{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85513d2b-af62-4daa-beb7-00ecfef0c464",
   "metadata": {},
   "source": [
    "# Task 1 Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75209df-e378-4656-86f7-2a95ba1e87e3",
   "metadata": {},
   "source": [
    "## Instruction on how to read the report for Task 1.Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bfa3a-c8fc-4761-aefc-ce8452157127",
   "metadata": {},
   "source": [
    "I create two local folders called `train_combined` and `test_combined` to store files that are from train test (include positive and negative) and test set (e.g. positive and negative); the train and test set are provided on `hdfs://hadoop.cdms.westernsydney.edu.au:9000/users/ugbigdata/Hadoop/imdb/tinyversion`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a947a-78e5-4eeb-b542-eea25b6c66ee",
   "metadata": {},
   "source": [
    "I create a remote folder called `train_test` which stores both filenames existed in train folder and test folder of the provided data (`tinyversion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa8861-491f-42b3-b39e-8dd4f6ac410f",
   "metadata": {},
   "source": [
    "I create a Hadoop mapreducer to calculate the TF-IDF value for every file in `train_test` . The TF mapper and reducer are two separated files called `TF_mapper.py` and `TF_reducer.py`. Similarly, I created another two python files called `IDF_mapper.py` and `IDF_reducer.py` to calculate the IDF value of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700097e-954b-488f-b6d9-dc0e6e12fc81",
   "metadata": {},
   "source": [
    "I have obtained both value of TF and IDF. I created another 2 python files which account for the final calculation of TF-IDF, called `onecode_TFIDF_mapper.py` and `onecode_TFIDF_reducer.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffd39e-5b5b-4dd2-b3fc-8bf500b89310",
   "metadata": {},
   "source": [
    " I created the last python file (which is named `onecode_TFIDF_output.py` which will return the wordlist, the rating score of each file, and the TF-IDF values of each words in each file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d84cc7-5020-4206-9e8a-2533f720efff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b4413-68d8-4433-b131-337405e99f3b",
   "metadata": {},
   "source": [
    "Create a separate `train_combined` and `test_combined` folder to check for filenames in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76099e-3fa0-4353-ac1b-e5118c79754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /users/ugbigdata/Hadoop/imdb/tinyversion/train/neg/* /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/tinyversion/train_combined\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /users/ugbigdata/Hadoop/imdb/tinyversion/train/pos/* /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/tinyversion/train_combined\n",
    "\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /users/ugbigdata/Hadoop/imdb/tinyversion/test/neg/* /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/tinyversion/test_combined\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /users/ugbigdata/Hadoop/imdb/tinyversion/test/pos/* /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/tinyversion/test_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168e633-b1df-45c7-b961-3c9e2221af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "train_folderfile = os.listdir(\"./tinyversion/train_combined/\")\n",
    "for i in range(len(train_folderfile)):\n",
    "file_realname, rating = train_folderfile[i].split(\"_\")\n",
    "train_folderfile[i] = file_realname\n",
    "test_folderfile = os.listdir(\"./tinyversion/test_combined/\")\n",
    "for i in range(len(test_folderfile)):\n",
    "file_realname, rating = test_folderfile[i].split(\"_\")\n",
    "test_folderfile[i] = file_realname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3acb487-f5ba-4a2b-9a4f-bec910a7b2f3",
   "metadata": {},
   "source": [
    "Create a combined `train_test` folder that stores files from both train and test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11b733-4ee1-4a2b-b1df-189048d67bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/local/hadoop/bin/hdfs dfs -rm -r /users/manan/Desktop/Big-Data/Assignment/tinyversion/train_test\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -mkdir /users/manan/Desktop/Big-Data/Assignment/tinyversion/train_test\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cp /users/manan/Desktop/Big-Data/Assignment/tinyversion/train/neg/* /users/manan/Desktop/Big-Data/Assignment/tinyversion/train_test\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cp /users/users/manan/Desktop/Big-Data/Assignment/tinyversion/train/pos/* /users/manan/Desktop/Big-Data/Assignment/tinyversion/train_test\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cp /users/manan/Desktop/Big-Data/Assignment/Assignment_1/tinyversion/test/neg/* /users/manan/Desktop/Big-Data/Assignment/tinyversion/train_test\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cp /users/manan/Desktop/Big-Data/Assignment/Assignment_1/tinyversion/test/pos/* /users/manan/Desktop/Big-Data/Assignment/tinyversion/train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbede3f-e31d-4fcb-ac5d-69346f4a5cc2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07df2b-61cd-4915-9c46-a4b2b1b8cf5f",
   "metadata": {},
   "source": [
    "### Code for IDF Mapper, Reducer and Code for TF Mapper, Reducer, as well as code for TF-IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce73222-9610-4b3d-b3be-4fd29bbca015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF Mapper\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "mapper_output = dict() #output of mapper.py\n",
    "term_count = 0 #for term count (second index output\n",
    "stop_words = set(stopwords.words('english')) #create a stopwords\n",
    "TERM_RE = re.compile(r\"[\\w']+\") #remove unnecessary symbols from the text file\n",
    "current_filename= \"\" #check current name of the txt file\n",
    "#remove number from the string\n",
    "def remove_number(input_str):\n",
    "pattern = r'\\d+'\n",
    "result = re.sub(pattern, '', input_str)\n",
    "return result\n",
    "#check for line input:\n",
    "for line in sys.stdin: #REMEMBER THAT this for loop will loop through every files, hence there will be repetition\n",
    "#Calculating the total number of documents\n",
    "filepath = os.environ[\"mapreduce_map_input_file\"]\n",
    "#filename = os.getenv(\"input_file\") #for local file\n",
    "filename = os.path.split(filepath)[-1]\n",
    "if current_filename != filename:\n",
    "current_filename = filename\n",
    "#remove all unnecessary symbol and number\n",
    "for term in TERM_RE.findall(line):\n",
    "if term not in stopwords:\n",
    "term = remove_number(term)\n",
    "term = term.strip()\n",
    "term = term.strip(\"'\")\n",
    "if len(term) > 1: #remove blank str and one word str\n",
    "if term.lower() in mapper_output.keys():\n",
    "mapper_output[term.lower()][0] += 1 #term count\n",
    "else:\n",
    "mapper_output[term.lower()] = [1, 1, current_filename] # index 1: term appear in # times in total document\n",
    "for key in mapper_output.keys():\n",
    "#this will print the term, term count, (future) term appear in # times in total document\n",
    "print(\",\".join([key, str(mapper_output[key][0]), str(mapper_output[key][1]), str(mapper_output[key][2])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb6696-e179-49f4-b6d7-bce1c21793ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF Reducer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "reducer_output = dict()\n",
    "doc_list = set() #distinct document list\n",
    "for line in sys.stdin:\n",
    "line = line.strip() #strip line from previous text output\n",
    "term, term_count, term_appear_1, doc_name = line.split(\",\")\n",
    "doc_list.add(doc_name)\n",
    "#calculate term count in total, and number of times terms appeared in document\n",
    "if term not in reducer_output.keys():\n",
    "reducer_output[term] = [int(term_count), 1] #we dont really need term count for IDF\n",
    "else:\n",
    "reducer_output[term][0] += int(term_count)\n",
    "reducer_output[term][1] += 1\n",
    "doc_list_len = len(doc_list)\n",
    "for key in reducer_output.keys():\n",
    "#return term, IDF of term\n",
    "print(\",\".join([key, str(math.log10(int(doc_list_len) / int(reducer_output[key][1])))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72a851-889e-4ee7-9cb9-ea13a77915e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF Mapper\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "TERM_RE = re.compile(r\"[\\w']+\")\n",
    "mapper_output = dict()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "current_filename = \"\"\n",
    "#remove number from the string\n",
    "def remove_number(input_str):\n",
    "pattern = r'\\d+'\n",
    "result = re.sub(pattern, '', input_str)\n",
    "return result\n",
    "#check for line input:\n",
    "for line in sys.stdin:\n",
    "filepath = os.environ[\"mapreduce_map_input_file\"]\n",
    "#filename = os.getenv('input_file')\n",
    "filename = os.path.split(filepath)[-1]\n",
    "if current_filename != filename:\n",
    "current_filename = filename\n",
    "#remove all unnecessary symbol and number\n",
    "for term in TERM_RE.findall(line):\n",
    "if term not in stopwords:\n",
    "term = remove_number(term)\n",
    "term = term.strip()\n",
    "term = term.strip(\"'\")\n",
    "if len(term) > 1: #remove blank str and one word str\n",
    "#calculate total number of term in a doc\n",
    "if term.lower() in mapper_output.keys():\n",
    "mapper_output[term.lower()][1] += 1 #term counts\n",
    "else:\n",
    "mapper_output[term.lower()] = [current_filename, 1] #filename, term counts\n",
    "for key in mapper_output.keys():\n",
    "print(\",\".join([str(mapper_output[key][0]), key, str(mapper_output[key][1])])) #output: filename, term, term counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38e479-3fd2-414b-94ac-c01bbbfb2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF Reducer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "filename_total_count = dict()\n",
    "distinct_term = set()\n",
    "test_dict = dict()\n",
    "for line in sys.stdin:\n",
    "line = line.strip()\n",
    "filename, term, term_count = line.split(\",\")\n",
    "distinct_term.add(term)\n",
    "#calculate the total # of term in a document\n",
    "if filename not in filename_total_count.keys():\n",
    "filename_total_count[filename] = int(term_count)\n",
    "else:\n",
    "filename_total_count[filename] += int(term_count)\n",
    "#creating a test_dict that stores value as a dict\n",
    "if filename not in test_dict.keys():\n",
    "test_dict[filename] = {term : int(term_count)}\n",
    "else:\n",
    "if term not in test_dict[filename].keys():\n",
    "test_dict[filename][term] = int(term_count)\n",
    "else:\n",
    "test_dict[filename][term] += int(term_count)\n",
    "#adding non-value for term\n",
    "for d_term in distinct_term:\n",
    "for filename_check in test_dict.keys():\n",
    "if d_term not in test_dict[filename_check].keys():\n",
    "test_dict[filename_check][d_term] = 0\n",
    "#sort dictionary\n",
    "for filename_check in test_dict.keys():\n",
    "test_dict[filename_check] = dict(sorted(test_dict[filename_check].items()))\n",
    "#print output\n",
    "for key in test_dict.keys():\n",
    "for term_key in test_dict[key].keys():\n",
    "print(\",\".join([key, term_key, str(int(test_dict[key][term_key]) / int(filename_total_count[key]))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add0599a-b9ea-408a-b0d1-b8614bd64c29",
   "metadata": {},
   "source": [
    "TF-IDF Mapper or `onecode_TFIDF_mapper.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881f055-cd40-4c65-9636-00e4fd3dfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Mapper\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "TF_dict = dict()\n",
    "IDF_dict = dict()\n",
    "distinct_term = set()\n",
    "for line in sys.stdin:\n",
    "line = line.strip()\n",
    "if len(line.split(',')) == 3: #to get the TF_dictionary\n",
    "filename, term, term_TF = line.split(\",\")\n",
    "if filename not in TF_dict.keys():\n",
    "TF_dict[filename] = {term : term_TF}\n",
    "else:\n",
    "if term not in TF_dict[filename].keys():\n",
    "TF_dict[filename][term] = term_TF\n",
    "else: #else it will read the IDF_dictionary\n",
    "term, term_IDF = line.split(',')\n",
    "if term not in IDF_dict.keys():\n",
    "IDF_dict[term] = term_IDF\n",
    "for key in IDF_dict.keys():\n",
    "print(','.join([key, str(eval(IDF_dict[key]))])) #print\n",
    "for key in TF_dict.keys():\n",
    "for term_key in TF_dict[key].keys():\n",
    "print(','.join([key, term_key, str(eval(TF_dict[key][term_key]))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad0eff-c4b7-4f43-9be1-5154d335effb",
   "metadata": {},
   "source": [
    "TF-IDF Reducer or `onecode_TFIDF_reducer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81b665-d481-4103-9b72-69d6ee23e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Reducer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "IDF_dict = dict()\n",
    "TF_dict = dict()\n",
    "TFIDF_dict = dict()\n",
    "for line in sys.stdin:\n",
    "line = line.strip()\n",
    "if len(line.split(',')) == 3: #TF_dict\n",
    "filename, term, TF_values = line.split(',')\n",
    "#calculate the value of TF-IDF: filename, every terms, TF-IDF\n",
    "if filename not in TFIDF_dict.keys():\n",
    "TFIDF_dict[filename] = {term : eval(TF_values) * eval(IDF_dict[term])}\n",
    "else:\n",
    "if term not in TFIDF_dict[filename].keys():\n",
    "TFIDF_dict[filename][term] = eval(TF_values) * eval(IDF_dict[term])\n",
    "else: #IDF_dict\n",
    "term, IDF_values = line.split(',')\n",
    "if term not in IDF_dict.keys():\n",
    "IDF_dict[term] = IDF_values\n",
    "for key in TFIDF_dict.keys():\n",
    "for term_key in TFIDF_dict[key].keys():\n",
    "print(','.join([key, term_key, str(TFIDF_dict[key][term_key])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883faab5-2ac2-448e-ab45-f6584c5ea485",
   "metadata": {},
   "source": [
    "`onecode_TFIDF_output.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59085a9c-2677-4cff-bdb0-75172989652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = open('train_test_output_local/local_TFIDF_mapreduce/part-00000', 'r')\n",
    "TFIDF_dict = dict()\n",
    "filename_and_rating_dict = dict()\n",
    "wordlist = []\n",
    "for line in TFIDF.readlines():\n",
    "line = line.strip()\n",
    "filename, term, TFIDF_values = line.split(',')\n",
    "file_realname, rating = filename.split('_')\n",
    "# wordlist\n",
    "if term not in wordlist:\n",
    "wordlist.append(term)\n",
    "# filename and rating dictionary\n",
    "if file_realname not in filename_and_rating_dict.keys():\n",
    "filename_and_rating_dict[file_realname] = int(rating)\n",
    "#filename, words and TFIDF values\n",
    "if file_realname not in TFIDF_dict.keys():\n",
    "TFIDF_dict[file_realname] = [eval(TFIDF_values)]\n",
    "else:\n",
    "TFIDF_dict[file_realname].append(eval(TFIDF_values))\n",
    "# return wordlist\n",
    "word_list = open('train_test_output_local/wordlist.txt', 'w')\n",
    "word_list.write(str(wordlist))\n",
    "word_list.close()\n",
    "#file and rating score\n",
    "rating_score = open('train_test_output_local/rating_score.txt', 'w')\n",
    "rating_score.write(str(filename_and_rating_dict))\n",
    "rating_score.close()\n",
    "# return filename and its TFIDF values for each word\n",
    "fileTFIDFdict = open('train_test_output_local/fileTFIDFdict.txt', 'w')\n",
    "fileTFIDFdict.write(str(TFIDF_dict))\n",
    "fileTFIDFdict.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b15d3-1879-482e-acf3-d9c39c5a0909",
   "metadata": {},
   "source": [
    "## TF-IDF computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad72f6-946a-47d4-b940-5dc8aaa9cf77",
   "metadata": {},
   "source": [
    "Compute TF and IDF calculation separately for every file in the `train_test folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800e3a7-764d-4c8c-b3a3-fc5678b25481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------IDF MapReduce\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /users/manan/Desktop/Big-Data/Assignment/Data_output/train_test_output/IDF_mapreduce/\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
    "-mapper IDF_mapper.py \\\n",
    "-reducer IDF_reducer.py \\\n",
    "-input /users/manan/Desktop/Big-Data/Assignment/tinyversion/train_test/* \\\n",
    "-output /users/manan/Desktop/Big-Data/Assignment/Data_output/train_test_output/IDF_mapreduce/ \\\n",
    "-file /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/IDF_mapper.py \\\n",
    "-file /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/IDF_reducer.py\n",
    "#-------------------------------------------------------------------TF MapReduce\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /users/manan/Desktop/Big-Data/Assignment/Data_output/train_test_output/TF_mapreduce/\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
    "-mapper TF_mapper.py \\\n",
    "-reducer TF_reducer.py \\\n",
    "-input /users/manan/Desktop/Big-Data/Assignment/tinyversion/train_test/* \\\n",
    "-output /users/manan/Desktop/Big-Data/Assignment/Data_output/train_test_output/TF_mapreduce/ \\\n",
    "-file /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/TF_mapper.py \\\n",
    "-file /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/TF_reducer.py\n",
    "#---------------------------------------------------------------TF-IDF Mapreduce\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm -r /users/manan/Desktop/Big-Data/Assignment/Data_output/train_test_output/TFIDF_mapreduce/\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
    "-mapper onecode_TFIDF_mapper.py \\\n",
    "-reducer onecode_TFIDF_reducer.py \\\n",
    "-input /users/manan/Desktop/Big-Data/Assignment/Data_output/train_test_output/IDF_mapreduce/part-00000 \\\n",
    "-input /users/manan/Desktop/Big-Data/Assignment/Data_output/train_test_output/TF_mapreduce/part-00000 \\\n",
    "-output /users/manan/Desktop/Big-Data/Assignment/Data_output/train_test_output/TFIDF_mapreduce/ \\\n",
    "-file /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/onecode_TFIDF_mapper.py \\\n",
    "-file /bigdata/users/student/mbhatia/Assignment/Hadoop_ver3/onecode_TFIDF_reducer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88cadce-2ee8-4d2e-9832-39c4a445b729",
   "metadata": {},
   "source": [
    "Use the last python file to return final values, which will be stored within the following name: `train_test_wordlist` for words list, `train_test_review_TFIDF` for TF-IDF results, `train_test_rating_score` for filenames and their correlating rating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d658fba-a38b-43ec-bd18-4eb34d1f18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 onecode_TFIDF_output.py #this python script output created 3 file name wordlist.txt, fileTFIDFdict.txt, rating_score.txt\n",
    "#store wordlist.txt into variable named wordlist\n",
    "f = open(\"train_test_output_local/wordlist.txt\")\n",
    "train_test_wordlist = eval(f.read())\n",
    "#store each review TFIDF as a dict into variable named review_TFIDF\n",
    "f = open(\"train_test_output_local/fileTFIDFdict.txt\")\n",
    "train_test_review_TFIDF = eval(f.read())\n",
    "#rating of each review\n",
    "f = open(\"train_test_output_local/rating_score.txt\")\n",
    "train_test_rating_score = eval(f.read()) #contain file realname and rating score as dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b60e89-26e2-446b-8286-98a0af6661a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2150938-95c4-4544-abc5-3e6cce89bd9c",
   "metadata": {},
   "source": [
    "# Task 2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6f17b-378b-46f4-855b-b42f7a1588c8",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
